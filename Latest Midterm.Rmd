---
title: "E-commerce Business Transaction Analysis"
author: "Team 3"
date: "March 16 2024"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
#knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

## Loading the Dataset

```{r}

# Reading the dataset

data = read.csv("Sales_Transaction.csv")

# Structure of the dataset

str(data)

```

## Summary of the dataset

```{r}
# Before Cleaning the Data

summary(data)
```

#### Summary:

1.  Dataset contain **536350** entries of row and **8** columns.
2.  Column 'Date' should be 'Date' format for better analysis.
3.  Column 'Quantity' contain negative value.
4.  The 'CustomerNo' column has some missing values.
5.  In TransactionNo column, some value have letter "C" in the first string. That mean **CANCELLATION.**

## Data Cleaning

#### Remove NA Values:

```{r}
# Remove rows with NA in dataset

data <- na.omit(data)

summary(data)

```

```{r}
# unique products count
unique_products_count <- length(unique(data$ProductNo))

# unique customers count
unique_customers_count <- length(unique(data$CustomerNo))

# unique countries  count
unique_countries_count <- length(unique(data$Country))

cat("Count of Unique Products:", unique_products_count, "\n")

cat("Count of Unique Customers:", unique_customers_count, "\n")

cat("Count of Unique Countries:", unique_countries_count, "\n")

#### Removing Duplicate Values:

# check for Duplicate Rows

sum(duplicated(data))

# Remove duplicate rows

library(dplyr)

data <- data %>% distinct()

# Checking for number of duplicated rows.

sum(duplicated(data))


```

```{r}
# Removing rows where 'TransactionNo' starts with 'C'

data <- data[!grepl("^C", data$TransactionNo),]

# Remove rows where 'Quantity' is less than 0

data <- data[data$Quantity >=0,]


```

```{r}
# Converts date strings to Date objects

data$Date <- as.Date(data$Date, format = "%m/%d/%Y")

# Convert character variables to factors

data$Country <- as.factor(data$Country)

```

#### Creating Features:

```{r}
# Make a column Revenue

data$Revenue <- data$Price * data$Quantity

library(lubridate)

# Day of the Week

data$DayofWeek <- wday(data$Date , label = TRUE)

# Make a Month column

data$Month <- month(data$Date, label=TRUE)

```

#### Summary of the Cleaned Dataset:

```{r}
# After cleaned the dataset

summary(data)
```

```{r}
# Structure of the dataset

str(data)
```

## **EXPLORATORY DATA ANALYSIS**

### Distribution of the variables:

```{r}

# Creating Histogram plot and Q-Q plot to see distribution of the Variables

library(ggplot2)


ggplot(data, aes(x=Price)) +
  geom_histogram(bins=30, fill='green', color = "red") +
  xlim(0,50)+
  labs(title="Distribution of Price", x="Price", y='density')
  
qqnorm(data$Price, main='Q-Q Plot of Price', col='blue')
qqline(data$Price, col='black')



  

```

The Q-Q plot of the 'Price' data indicates a deviation from the normal distribution. The central values align closely with the theoretical line, suggesting normality in the mid-range of the data.

```{r}
ggplot(data, aes(x=Revenue)) +
  geom_histogram(bins=30, fill='green', color = "red") +
  xlim(0,1000)+
  labs(title="Distribution of Revenue", x="Revenue", y='density')
  
qqnorm(data$Revenue, main='Q-Q Plot of Revenue', col='blue')
qqline(data$Revenue, col='black')
```

In these graphs, the data are not normally distributed and contain outliers on the higher end of the scale.

#### Removing Outliers:

```{r}
library("ggplot2")

#removing quantity outliers
data2 = outlierKD2(data, Quantity, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in quantity 
na_quantity <- sum(is.na(data2$Quantity))
cat("Number of NA values in Quantity:", na_quantity, "\n")

#removing price outliers
data3 = outlierKD2(data2, Price, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in price
na_price <- sum(is.na(data3$Price))
cat("Number of NA values in Price:", na_price, "\n")

#removing revenue outliers
new_data = outlierKD2(data3, Revenue, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in revenue 
na_revenue <- sum(is.na(new_data$Revenue))
cat("Number of NA values in revenue:", na_revenue, "\n")


#after  the outliers are removed , they  are replaced with NA values hence  removing the rows containing those NA values

new_data<- na.omit(new_data)

summary(new_data)

```

## SMART QUESTIONS

#### Q1 How many Unique products does the customer purchase in each transaction?

```{r}
# Load the dplyr package
library(dplyr)

# Group the data by CustomerNo and TransactionNo, then count the number of distinct products in each group
product_counts <- new_data %>%
  group_by(CustomerNo, TransactionNo) %>%
  summarise(DistinctProducts = n_distinct(ProductNo), .groups = 'drop')

product_counts
```

#### Q2 What are the most frequently purchased products in our dataset, and is there a statistically significant difference in the Purchase Quantity among these top products?

```{r}

# Identifying the most frequently purchased products
Product_frequencies <- new_data %>%
  group_by(ProductNo,ProductName)%>%
  count(ProductNo, sort=TRUE)

# Select the top 10 frequently purchased products
top_products <- head(Product_frequencies,10)

top_products
```

```{r}
# Plotting the frequencies using bar chart

ggplot(top_products, aes(x = n,  y = reorder(ProductNo, n))) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Top 10 Most Frequently Purchased Products",
       x = "Frequency ",
       y = "Product No") 


```

The above Bar chart visualizes the top 10 most frequently purchased products. We conduct statistical test to determine if the differences in purchase quantities among these top products are significant, considering that the quantity data be normally distributed.

We did ANOVA test these variables.

```{r}
# ANOVA test to determine if statistically significance difference in the purchase Quantity among these top products

# Filter the dataset to include only the top 10 products
filtered_data <- new_data %>%
  filter(ProductNo %in% top_products$ProductNo)


# Null Hypothesis(H0) : There is no difference in average Purchase Quantity among the top 10 most frequently purchased products.

# Alternative Hypothesis(H1) : At least one of the top 10 products has a different average Purchase Quantity compared to others.



anova_result <- aov(Quantity ~ ProductNo, data = filtered_data)

summary(anova_result)


```

The P-value(2e-16) \< 0.05 suggest that we are rejecting the null hypothesis of the ANOVA test. This means that there is a significant difference in the mean of purchased quantity of top products.

We need to perform post-hoc test , to determined this which ones have means that are significantly different from each other.

```{r}
# Perform Tukey's HSD test


tukey_result <- TukeyHSD(anova_result)

tukey_result



```

1.**Pairs with Significant Differences:** Whenever the p-value (p-adj) is less than 0.05, and the confidence interval (lwr,upr) not have zero, we can say there is a significant difference in purchase quantity between the two products. For eg, the comparison 21931-21212 shows a difference of -7.3696 with a p-value \<0.001 which indicates that Product 21931 is purchased in quantities that are lower than Product 21212.

**2.Pairs without Significant Differences:** When confidence interval includes zero or the p-value is greater than 0.05, it meansd that there's not a significant difference in Purchase Quantity between the two products. For eg, product pair 22411-21931 with a difference of 0.0840 and p-value of 1.000 indicates no significant difference in purchase quantity between these two products.

**3.Highly Significant Differences:** For products pairs which have a very small p-value (\<0.01), they are considered to have a highly significant difference in Purchase Quantity.

#### Q3.How do monthly and Quaterly sales trends vary within a year ? Is there a statistically significant difference in the distribution of the number of sales transactions across different quarters of the year?"

```{r}

# Extract Month and Quarter from Date 
new_data$Month <- format(new_data$Date, "%Y-%m")
new_data$Quarter <- quarters(new_data$Date)

# Aggregate sales by month
monthly_sales <- aggregate(Revenue ~ Month, data=new_data, FUN=sum)

# Aggregate sales by quarter
quarterly_sales <- aggregate(Revenue ~ Quarter, data=new_data, FUN=sum)

# Plot for Monthly Sales Trends
ggplot(monthly_sales, aes(x=Month, y=Revenue)) + 
  geom_bar(stat="identity", fill="skyblue") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title="Monthly Sales Trends", x="Month", y="Total Sales")

# Plot for Quarterly Sales Trends
ggplot(quarterly_sales, aes(x = Quarter, y = Revenue )) +
  geom_bar(stat = "identity", fill = "lightgreen") +
  labs(title = "Quarterly Sales Trends", x = "Quarter", y = "Total Sales") +
  theme_minimal()


```

We did Chi-squared test to determine significant difference in the number of transaction across different quarters of the year.

```{r}

# Count the number of transactions per quarter

transaction_counts_by_quarter <- table(new_data$Quarter)

transaction_counts_by_quarter
```

```{r}

# Null Hypothesis(H0) : There is  significant difference in the  number of transactions across different quarters of the year.

# Alternative Hypothesis(H1) : There is a significant difference in the number of sales transactions across different quarters of the year.

# Perform the Chi-squared test
chisquared_test <- chisq.test(transaction_counts_by_quarter)

chisquared_test
```

The P-value (2e-16) \< 0.05, indicating that there is a statistically significant difference in the distribution of the number of sales transactions across different quarters , suggesting seasonality factors affect sales volume.

#### Q4. How do average transaction values vary across different countries in our sales dataset ?

```{r}

dataCountrytop <- new_data %>%
  group_by(Country) %>%
  summarise(Revenue = sum(Revenue)) %>%
  arrange(desc(Revenue))

top_10_countries <- head(dataCountrytop,10)

top_10_countries


```

```{r}
ggplot(top_10_countries, aes(x = reorder(Country, Revenue), y = Revenue, fill = Revenue)) +
  geom_bar(stat = "identity") +
  labs(title = "Highest Revenue Countries", x = "Country", y = "Revenue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_gradient(low = "blue", high = "red") + 
  coord_flip()
```
#### Q5.What proportion of revenue is contributed by high-value customers, and how does their spending behavior compare to the overall customer base?
```{r}

threshold <- quantile(new_data$Revenue, 0.95)
high_val_customer <- unique(new_data$CustomerNo[new_data$Revenue > threshold])
length(high_val_customer)

high_val_transactions <- subset(new_data, CustomerNo %in% high_val_customer)
avg_transaction_amt <- mean(high_val_transactions$Revenue)
avg_transaction_totalamt <- mean(new_data$Revenue)
high_val_transactions

avg_transaction_amt
avg_transaction_totalamt

ggplot(new_data, aes(x = Revenue)) +
  geom_histogram(fill = "skyblue", color = "black", bins = 40) +
  geom_vline(xintercept = threshold, color = "orange", linetype = "dashed", size = 2) +
  labs(x = "Revenue", y = "Frequency", title = "Distribution of Customer Spending") +
  theme_minimal()

ggplot(new_data, aes(x = CustomerNo, y = Revenue, fill = CustomerNo %in% high_val_customer)) +
  geom_boxplot() +
  labs(x = "Customer Number", y = "Revenue", title = "Distribution of Revenue for High-Value Customers") +
  scale_fill_manual(values = c("FALSE" = "orange", "TRUE" = "blue")) +
  theme_minimal() +
  theme(legend.position = "none")

ggplot(new_data, aes(y = Country, x = Revenue, fill = CustomerNo %in% high_val_customer)) +
  geom_bar(stat = "summary", fun = "mean", position = "dodge") +
  labs(y = "Country", x = "Revenue", title = "Average Revenue by Country") +
  scale_fill_manual(values = c("FALSE" = "orange", "TRUE" = "blue")) +
  theme_minimal() +
  theme(legend.position = "none")

total_revenue <- sum(new_data$Revenue)
high_value_revenue <- sum(subset(new_data, CustomerNo %in% high_val_customer)$Revenue)
high_value_contribution <- (high_value_revenue / total_revenue) * 100

cat("High-Value Customer Revenue Contribution:", high_value_contribution, "%")
```