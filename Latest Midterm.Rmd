---
title: "E-commerce Business Transaction Analysis"
author: "Team 3"
date: "March 16 2024"
# date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
knitr::opts_chunk$set(warning = F, results = "markup", message = F)
#knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

## Loading the Dataset

```{r}

# Reading the dataset

data = read.csv("Sales_Transaction.csv")

# Structure of the dataset

str(data)

```

## Summary of the dataset

```{r}
# Before Cleaning the Data

summary(data)
```

#### Summary:

1.  Dataset contain **536350** entries of row and **8** columns.
2.  Column 'Date' should be 'Date' format for better analysis.
3.  Column 'Quantity' contain negative value.
4.  The 'CustomerNo' column has some missing values.
5.  In TransactionNo column, some value have letter "C" in the first string. That mean **CANCELLATION.**

## Data Cleaning

#### Remove NA Values:

```{r}
# Remove rows with NA in dataset

data <- na.omit(data)

summary(data)

```

```{r}
# unique products count
unique_products_count <- length(unique(data$ProductNo))

# unique customers count
unique_customers_count <- length(unique(data$CustomerNo))

# unique countries  count
unique_countries_count <- length(unique(data$Country))

cat("Count of Unique Products:", unique_products_count, "\n")

cat("Count of Unique Customers:", unique_customers_count, "\n")

cat("Count of Unique Countries:", unique_countries_count, "\n")

#### Removing Duplicate Values:

# check for Duplicate Rows

sum(duplicated(data))

# Remove duplicate rows

library(dplyr)

data <- data %>% distinct()

# Checking for number of duplicated rows.

sum(duplicated(data))


```

```{r}
# Removing rows where 'TransactionNo' starts with 'C'

data <- data[!grepl("^C", data$TransactionNo),]

# Remove rows where 'Quantity' is less than 0

data <- data[data$Quantity >=0,]


```

```{r}
# Converts date strings to Date objects

data$Date <- as.Date(data$Date, format = "%m/%d/%Y")

# Convert character variables to factors

data$Country <- as.factor(data$Country)

```

#### Creating Features:

```{r}
# Make a column Revenue

data$Revenue <- data$Price * data$Quantity

library(lubridate)

# Day of the Week

data$DayofWeek <- wday(data$Date , label = TRUE)

# Make a Month column

data$Month <- month(data$Date, label=TRUE)

```

#### Summary of the Cleaned Dataset:

```{r}
# After cleaned the dataset

summary(data)
```

```{r}
# Structure of the dataset

str(data)
```

## **EXPLORATORY DATA ANALYSIS**

### Distribution of the variables:

```{r}

# Creating Histogram plot and Q-Q plot to see distribution of the Variables

library(ggplot2)


ggplot(data, aes(x=Price)) +
  geom_histogram(bins=30, fill='green', color = "red") +
  xlim(0,50)+
  labs(title="Distribution of Price", x="Price", y='density')
  
qqnorm(data$Price, main='Q-Q Plot of Price', col='blue')
qqline(data$Price, col='black')



  

```

The Q-Q plot of the 'Price' data indicates a deviation from the normal distribution. The central values align closely with the theoretical line, suggesting normality in the mid-range of the data.

```{r}

ggplot(data, aes(x=Quantity)) +
  geom_histogram(bins=30, fill='green', color = "red") +
  labs(title="Distribution of Quantity", x="Quantity", y='Density')
  
qqnorm(data$Quantity, main='Q-Q Plot of Price', col='blue')
qqline(data$Quantity, col='black')

```

```{r}
ggplot(data, aes(x=Revenue)) +
  geom_histogram(bins=30, fill='green', color = "red") +
  xlim(0,1000)+
  labs(title="Distribution of Revenue", x="Revenue", y='density')
  
qqnorm(data$Revenue, main='Q-Q Plot of Revenue', col='blue')
qqline(data$Revenue, col='black')
```

In these graphs, the data are not normally distributed and contain outliers on the higher end of the scale.

#### Removing Outliers:

```{r}
library("ggplot2")

#removing quantity outliers
data2 = outlierKD2(data, Quantity, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in quantity 
na_quantity <- sum(is.na(data2$Quantity))
cat("Number of NA values in Quantity:", na_quantity, "\n")

#removing price outliers
data3 = outlierKD2(data2, Price, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in price
na_price <- sum(is.na(data3$Price))
cat("Number of NA values in Price:", na_price, "\n")

#removing revenue outliers
new_data = outlierKD2(data3, Revenue, rm=TRUE, boxplt=FALSE, histogram=FALSE, qqplt=TRUE) 

# Count of NA values in revenue 
na_revenue <- sum(is.na(new_data$Revenue))
cat("Number of NA values in revenue:", na_revenue, "\n")


#after  the outliers are removed , they  are replaced with NA values hence  removing the rows containing those NA values

new_data<- na.omit(new_data)

summary(new_data)

```

## SMART QUESTIONS

#### Q1 How many Unique products does the customer purchase in each transaction?

```{r}
# Load the dplyr package
library(dplyr)

# Group the data by CustomerNo and TransactionNo, then count the number of distinct products in each group
product_counts <- new_data %>%
  group_by(CustomerNo, TransactionNo) %>%
  summarise(DistinctProducts = n_distinct(ProductNo), .groups = 'drop')

product_counts
```

#### Q2 What are the most frequently purchased products in our dataset, and is there a statistically significant difference in the Purchase Quantity among these top products?

```{r}

# Identifying the most frequently purchased products
Product_frequencies <- new_data %>%
  group_by(ProductNo,ProductName)%>%
  count(ProductNo, sort=TRUE)

# Select the top 10 frequently purchased products
top_products <- head(Product_frequencies,10)

print(top_products)
```

```{r}
# Plotting the frequencies using bar chart

ggplot(top_products, aes(x = n,  y = reorder(ProductNo, n))) +
  geom_bar(stat = "identity", fill = "blue") +
  labs(title = "Top 10 Most Frequently Purchased Products",
       x = "Frequency ",
       y = "Product No") 


```

```{r}
# ANOVA test to determine if statistically significance difference in the purchase Quantity among these top products

# Filter the dataset to include only the top 10 products
filtered_data <- new_data %>%
  filter(ProductNo %in% top_products$ProductNo)


# Null Hypothesis(H0) : There is no difference in average Purchase Quantity among the top 10 most frequently purchased products.

# Alternative Hypothesis(H1) : At least one of the top 10 products has a different average Purchase Quantity compared to others.

anova_result <- aov(Quantity ~ ProductNo, data = filtered_data)

summary(anova_result)


```

The P-value(2e-16) \< 0.05 suggest that we are rejecting the null hypothesis of the ANOVA test. This means that there is a significant difference in the mean of purchased quantity of top products.

We need to perform post-hoc test , to determined this which ones have means that are significantly different from each other.

```{r}
# Perform Tukey's HSD test


tukey_result <- TukeyHSD(anova_result)

tukey_result



```

1.**Pairs with Significant Differences:** Whenever the p-value (p-adj) is less than 0.05, and the confidence interval (lwr,upr) not have zero, we can say there is a significant difference in purchase quantity between the two products. For eg, the comparison 21931-21212 shows a difference of -7.3696 with a p-value \<0.001 which indicates that Product 21931 is purchased in quantities that are lower than Product 21212.

**2.Pairs without Significant Differences:** When confidence interval includes zero or the p-value is greater than 0.05, it meansd that there's not a significant difference in Purchase Quantity between the two products. For eg, product pair 22411-21931 with a difference of 0.0840 and p-value of 1.000 indicates no significant difference in purchase quantity between these two products.

**3.Highly Significant Differences:** For products pairs which have a very small p-value (\<0.01), they are considered to have a highly significant difference in Purchase Quantity.

#### Q3.How do monthly and Quaterly sales trends vary within a year ?

```{r}
 
new_data$Month <- format(new_data$Date, "%Y-%m")
new_data$Quarter <- quarters(new_data$Date)

monthly_sales <- aggregate(Revenue ~ Month, data=new_data, FUN=sum)

ggplot(monthly_sales, aes(x=Month, y=Revenue)) + 
  geom_bar(stat="identity", fill="skyblue") +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  labs(title="Monthly Sales Trends", x="Month", y="Total Sales")

quarterly_sales <- aggregate(Revenue ~ Quarter, data=new_data, FUN=sum)

ggplot(quarterly_sales, aes(x = Quarter, y = Revenue )) +
  geom_bar(stat = "identity", fill = "skyblue") +
  labs(title = "Quarterly Sales Trends", x = "Quarter", y = "Total Sales") +
  theme_minimal()


```

#### Q4. How do average transaction values vary across different countries in our sales dataset, and is this variation statistically significant?

```{r}

dataCountrytop <- new_data %>%
  group_by(Country) %>%
  summarise(Revenue = sum(Revenue)) %>%
  arrange(desc(Revenue))

top_10_countries <- head(dataCountrytop,10)

top_10_countries


```

```{r}
ggplot(top_10_countries, aes(x = reorder(Country, Revenue), y = Revenue, fill = Revenue)) +
  geom_bar(stat = "identity") +
  labs(title = "Highest Revenue Countries", x = "Country", y = "Revenue") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) + 
  scale_fill_gradient(low = "blue", high = "red") + 
  coord_flip()
```
